import requests
from bs4 import BeautifulSoup
import random
import json
import time
import os
from libs.kToolLibs import kMD5FileManager
import kCustomNotify

# =============================
# ç™½åå•é…ç½®è¯´æ˜ï¼š
# whitekeylist ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé…ç½®é¡¹ï¼ŒåŒ…å«ï¼š
# - keywordsï¼šå…³é”®è¯åˆ—è¡¨ï¼Œè¡¨ç¤ºå¿…é¡»å‘½ä¸­è¿™äº›å…³é”®è¯ç»„ï¼ˆæ¯ç»„æ˜¯â€œæˆ–â€å…³ç³»ï¼Œç»„ä¸ç»„ä¹‹é—´æ˜¯â€œä¸â€å…³ç³»ï¼‰
# - "keywords": [["è‹¹æœ", "iPhone"], ["16"]],è¡¨ç¤º è‹¹æœ 16ï¼Œiphone 16éƒ½æ˜¯å…³é”®å­—
# - exclude_keywordsï¼šæ’é™¤å…³é”®è¯ç»„ï¼Œæ¯ç»„æ˜¯â€œä¸â€å…³ç³»ï¼Œç»„ä¹‹é—´æ˜¯â€œæˆ–â€å…³ç³»
# - "exclude_keywords": [["16e"], ["äºŒæ‰‹", "æ‰‹æœº"]],è¡¨ç¤ºæ’é™¤åŒ…å«16e å’Œ åŒ…å« äºŒæ‰‹ä¸”æ‰‹æœº
# - price_rangeï¼šä¸€ä¸ªäºŒå…ƒç»„ (min_price, max_price)ï¼Œè¡¨ç¤ºä»·æ ¼èŒƒå›´ï¼ˆå…ƒï¼‰
# =============================

whitekeylist = [
    {
        "keywords": [["è‹¹æœ", "iPhone"], ["16"],["256","512","1T"]],
        "exclude_keywords": [["16e"],["MAC"], ["äºŒæ‰‹"],],
        "price_range": (1000, 4000)
    },
    {
        "keywords": [["åä¸º", "huawei"], ["p70","pura 70","mate 70","mate 60"],["256","512","1T"]],
        "exclude_keywords": [["16e"], ["äºŒæ‰‹"],],
        "price_range": (1000, 4000)
    },
    {
        "keywords": [["èœç±½æ²¹","é£Ÿç”¨æ²¹","ç‰ç±³æ²¹",],["5L"],],
        "exclude_keywords": [],
        "price_range": (0, 35)
    },
    {
        "keywords": [["å¤§ç±³","é‡‘é¾™é±¼","ç¦ä¸´é—¨","äº”å¸¸","æŸ´ç«å¤§é™¢" ],["5kg"],],
        "exclude_keywords": [],
        "price_range": (0, 15)
    },
    {
        "keywords": [["æ²æµ´éœ²", ],["èˆ’è‚¤ä½³"],],
        "exclude_keywords": [],
        "price_range": (0, 10)
    }
]

# éšæœº UA
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)...",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...",
    "Mozilla/5.0 (Linux; Android 10; SM-G970F)...",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
    "Mozilla/5.0 (Linux; Android 11; M2007J17C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Mobile Safari/537.36 Xiaomi",
    "Mozilla/5.0 (Linux; Android 10; ELS-AN00 Build/HUAWEIELS-AN00) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Mobile Safari/537.36 Huawei",
    "Mozilla/5.0 (Linux; Android 10; V1981A) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.110 Mobile Safari/537.36 Vivo",
    "Mozilla/5.0 (Linux; Android 11; Redmi Note 10 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; SAMSUNG SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/17.0 Chrome/96.0.4664.45 Mobile Safari/537.36"
]

# ğŸ“¦ ä»æ–‡ä»¶è¯»å–ä»£ç†
def load_proxies(filename='valid_proxies.json'):
    proxies = []
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    proxy = f"{item['type']}://{item['ip']}:{item['port']}"
                    proxies.append(proxy)
        except Exception as e:
            print("è¯»å–ä»£ç†æ–‡ä»¶å¤±è´¥ï¼š", e)
    return proxies

# ğŸ” åŒ¹é…åŒ…å«å…³é”®è¯
def match_keywords(title, keywords):
    title_cleaned = title.replace(" ", "").lower()
    return all(
        any(k.replace(" ", "").lower() in title_cleaned for k in group)
        for group in keywords
    )

# âŒ åŒ¹é…æ’é™¤å…³é”®è¯
def match_excludes(title, exclude_keywords):
    title_cleaned = title.replace(" ", "").lower()
    return any(
        all(k.replace(" ", "").lower() in title_cleaned for k in group)
        for group in exclude_keywords
    )

# ğŸŒ è¯·æ±‚ç½‘é¡µ
def get_html(url, proxy_list, max_retries=3):
    for attempt in range(max_retries):
        use_proxy = random.random() < 0.7 and proxy_list
        proxy = random.choice(proxy_list) if use_proxy else None
        proxies = {"http": proxy, "https": proxy} if proxy else None
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        try:
            print(f"[è¯·æ±‚å°è¯• {attempt + 1}] ä½¿ç”¨ä»£ç†ï¼š{bool(proxy)} - {proxy or 'ç›´è¿'}")
            response = requests.get(url, headers=headers, proxies=proxies, timeout=10)
            if response.status_code == 200:
                return response.text
            else:
                print(f"[çŠ¶æ€ç å¼‚å¸¸] {response.status_code}")
        except Exception as e:
            print(f"[è¯·æ±‚å¼‚å¸¸] {e}")
        time.sleep(1)
    print("[è¯·æ±‚å¤±è´¥] å¤šæ¬¡é‡è¯•åæ”¾å¼ƒã€‚")
    return None

# ğŸ•¸ ä¸»çˆ¬è™«é€»è¾‘
def crawl_smzdm():
    #https://www.smzdm.com/jingxuan/p2/  è¡¨ç¤ºç¬¬äºŒé¡µ
    url = "https://www.smzdm.com/jingxuan/"
    proxy_list = None
    #proxy_list = load_proxies()
    html = get_html(url, proxy_list)
    if not html:
        return []

    soup = BeautifulSoup(html, 'html.parser')
    li_tags = soup.find_all("li", class_="J_feed_za feed-row-wide")
    if not li_tags:
        print("æœªæ‰¾åˆ° li æ ‡ç­¾ (class=J_feed_za feed-row-wide)")
        return []

    results = []
    for li in li_tags:
        content_div = li.find("div", class_="z-feed-content")
        if not content_div:
            print("æœªæ‰¾åˆ° div æ ‡ç­¾ (class=z-feed-content)")
            continue

        # è·å–æ ‡é¢˜ + é“¾æ¥
        h5 = content_div.find("h5", class_="feed-block-title")
        if not h5:
            print("æœªæ‰¾åˆ° h5 æ ‡ç­¾ (class=feed-block-title)")
            continue
        title_a = h5.find("a")
        if not title_a:
            print("æœªæ‰¾åˆ°æ ‡é¢˜ a æ ‡ç­¾ (class=feed-block-title ä¸‹)")
            continue
        title = title_a.get_text(strip=True)
        href = title_a.get("href", "")

        # è·å–ä»·æ ¼
        price_a = content_div.find("a", class_="z-highlight")
        if not price_a:
            print("æœªæ‰¾åˆ°ä»·æ ¼ a æ ‡ç­¾ (class=z-highlight)")
            continue
        price_str = price_a.get_text(strip=True).replace("ï¿¥", "").replace(",", "")
        try:
            price = float(''.join(c for c in price_str if c.isdigit() or c == '.'))
        except Exception as e:
            print(f"ä»·æ ¼è½¬æ¢å¤±è´¥ï¼š{price_str} - é”™è¯¯ï¼š{e}")
            continue

        # è·å–â€œå€¼â€
        zhi_a = content_div.find("a", class_="J_zhi_like_fav price-btn-up")
        zhiV = None
        if zhi_a:
            zhi_span = zhi_a.find("span", class_="unvoted-wrap")
            if zhi_span and zhi_span.span:
                zhiV = zhi_span.span.get_text(strip=True)
            else:
                print("æœªæ‰¾åˆ° span å€¼æ ‡ç­¾ (class=unvoted-wrap)")
        else:
            print("æœªæ‰¾åˆ° a æ ‡ç­¾ (class=J_zhi_like_fav price-btn-up)")

        # è·å–â€œä¸å€¼â€
        buzhi_a = content_div.find("a", class_="J_zhi_like_fav price-btn-down")
        buzhiV = None
        if buzhi_a:
            buzhi_span = buzhi_a.find("span", class_="unvoted-wrap")
            if buzhi_span and buzhi_span.span:
                buzhiV = buzhi_span.span.get_text(strip=True)
            else:
                print("æœªæ‰¾åˆ° span ä¸å€¼æ ‡ç­¾ (class=unvoted-wrap)")
        else:
            print("æœªæ‰¾åˆ° a æ ‡ç­¾ (class=J_zhi_like_fav price-btn-down)")

        # å…³é”®è¯åŒ¹é…
        matched = False
        for rule in whitekeylist:
            if match_keywords(title, rule["keywords"]) and not match_excludes(title, rule.get("exclude_keywords", [])):
                min_p, max_p = rule.get("price_range", (0, float('inf')))
                if min_p <= price <= max_p:
                    matched = True
                    break
        if not matched:
            continue

        item = {
            "title": title,
            "href": href,
            "price": str(price),
            "zhi": zhiV,
            "buzhi": buzhiV
        }
        results.append(item)

    return results

# âœ… å¯åŠ¨å…¥å£
if __name__ == '__main__':
    product_data = crawl_smzdm()
    file_manager = kMD5FileManager('smzdmMonitorMD5.txt')
    # æ‰“å°æ‰€æœ‰å•†å“çš„å­—å…¸æ•°æ®
    if product_data:
        print(f"[çˆ¬å–å®Œæˆ] è·å–åˆ° {len(product_data)} æ¡å•†å“æ•°æ®:")
        for product in product_data:
            if file_manager.write_md5_with_date(product.get('title', '') + product.get('price', '')):
                notifytxt = f"æ ‡é¢˜:{product.get('title', '')}\nå•ä»·:{product.get('price', '')}\n é“¾æ¥:{product.get('href', '')}" 
                print(notifytxt)
                kCustomNotify.send_wecom_notification("SMZDMæé†’",notifytxt,"WECOM_BOT_GENERALNOTIFY_KEY")
            else:
                pass
                #print("md5 write failed")
    else:
        pass
